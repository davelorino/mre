---
title: ""
author: "Davide Lorino"
date: "2023-06-08"
output: 
  github_document:
    toc: true
---

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(dplyr)
library(purrr)
library(tidyr)
library(stringr)
library(knitr)
library(tibble)
```

# Hierarchical Beam Search for the Most Relevant Explanation in Bayesian Networks

First introduced in <i>Yuan, C. et al. (2011), <a href="https://arxiv.org/abs/1401.3893">"Most Relevant Explanations in Bayesian Networks"</a>, Journal of ArtiÔ¨Åcial Intelligence Research, pp. 309-352</i> - the Most Relevant Expalantion in a Bayesian network is the partial instantiation of nodes that maximizes the Generalized Bayes Factor. 

# Generalized Bayes Factor

The Generalized Bayes Factor is introduced by Yuan et al in the aforementioned paper. The GBF is a deterministic function of the Weight of Evidence (WoE) first introduced by Alan Turing. The WoE is the logarithm of the GBF (see Good, I. J. (1985), <a href= "https://www.cs.tufts.edu/~nr/cs257/archive/jack-good/weight-of-evidence.pdf">"Weight of Evidence: A Brief Survey"</a>, Bayesian Statistics 2, pp. 249-270, for more general discussion of history and concepts).

## Maximum Relevance - Reckless Drivers Example

The main value of a Generalized Bayes Factor is that it solves the problem that arises when fitting predictive models or any type of correlation analysis that focuses on maximum likelihood. As a toy example, lets imagine that in the space of one year, approximately 10,000 people are injured in car accidents. Surfacing the most common characteristics among people in car accidents through a maximum likelihood framework would generate insights like "people who breathe oxygen are most likely to be involved in a car accident" - what maximum likelihood doesn't account for is the fact that <i>anyone</i> is highly likely to breathe oxygen, regardless of whether they are involved in a car accident, so the association between "breathing oxygen" and "being in a car accident" is spurious. 

The Generalized Bayes Factor takes the approach of representing associations between events and hypotheses as the <i>posterior odds of observing a hypothesis</i> (e.g. breathing oxygen) under the condition that we observe some evidence (being in a car accident), vs observing that same hypothesis under the <i>negation of that hypothesis</i> (not being in a car accident).

To extend this analogy more concretely, observe the table below:

```{r echo=FALSE}


# create a synthetic dataset of age, gender and income
set.seed(42)
n <- 20
breathes_oxygen <- factor(sample(c("yes"), n, replace = TRUE, prob = c(1)))
reckless_driver <- factor(sample(c("yes", "no"), n, replace = TRUE, prob = c(0.20, 0.80)))

# define probabilities for income based on age and gender
accident_probs <- function(oxygen_val, reckless_val) {
  if (reckless_val == "yes") {
    return(c("yes" = 0.80, "no" = 0.20))
  }  else {
    return(c("yes" = 0.20, "no" = 0.80))
  }
}

# generate income variable
involved_in_an_accident <- factor(mapply(function(o, r) {
  sample(c("yes", "no"), 1, replace = TRUE, prob = accident_probs(o, r))
}, breathes_oxygen, reckless_driver))

# join age, gender and income into a dataframe
raw_data <- data.frame(breathes_oxygen, reckless_driver, involved_in_an_accident)

knitr::kable(raw_data)
```

In the table we can see a sample of individuals and information about whether or not they have been involved in a car accident, and whether these people breathe oxygen and/or are reckless drivers.

Below are the counts for each variable:

```{r echo=FALSE}

knitr::kable(raw_data %>%
  dplyr::mutate(reckless_driver_counts = dplyr::case_when(reckless_driver == "yes" ~ 1, TRUE ~ 0)) %>%
  dplyr::mutate(breathes_oxygen_counts = dplyr::case_when(breathes_oxygen == "yes" ~ 1, TRUE ~ 0)) %>%
  dplyr::group_by(involved_in_an_accident) %>%
  dplyr::summarise(
    `Number of People` = sum(breathes_oxygen_counts),
    `Reckless Drivers` = paste0(round((sum(reckless_driver_counts) / n()) * 100, 2), "%"),
    `Number of Reckless Drivers` = sum(reckless_driver_counts),
    `People who Breathe Oxygen` = paste0(round((sum(breathes_oxygen_counts) / n()) * 100, 2), "%"),
    `Number of People who Breathe Oxygen` = sum(breathes_oxygen_counts)
  ) %>%
  dplyr::rename(`Involved in an Accident` = involved_in_an_accident)
)
  
```

In this example, if we try to surface the features most commonly associated with being involved in an accident through maximum likelihood / correlation, we would conclude that breathing oxygen is more likely to be associated with being in an accident (100%) than being a reckless driver (70%). 

But if we observe the 'Most Relevant Explanation' by checking which hypothesis maximises the Generalized Bayes Factor, then we see a different story:

```{r echo=FALSE}
# define the targets that you want explanations for 
target_variable = "involved_in_an_accident"
target_value = "yes"

message_output <- capture.output(res <- mre::mre_hierarchical_beam_search(
  x = raw_data, 
  target_variable = target_variable, 
  target_value = target_value
  )
)

# show the candidate hypotheses with the greatest Generalized Bayes Factors
knitr::kable(res)
```

It turns out that breathing oxygen adds no additional information to explanation of being involved in an accident when the information about whether or not someone is a reckless driver is taken into account, even when the presence of breathing oxygen is 100% vs being a reckless driver which is present in only 70% of people involved in accidents.

The reason we are able to deduce that being a reckless driver is the Most <i>Relevant</i> Explanation is because it maximises the posterior odds of the hypothesis given that an accident happened, relative to the posterior odds of the hypothesis given that an accident did not happen. Breathing oxygen does not maximise the same quantity because it is a characteristic that is equally present among people involved in accidents and people not involved in accidents. That is the essence of an explanation / hypothesis which maximises the Generalized Bayes Factor.

## Exhaustive Search Complexity

The challenge of calculating the Generalized Bayes Factor is that it requires the posterior odds to be calculated for every possible partial instantiation of nodes in the network. Partial instantiations of Bayesian Networks is an NP-hard problem where the number of partial instantiations grows super-exponentially with each node and node-state in the network.

In concrete terms, if we assume a highly optimized algorithm that can calculate Generalized Bayes Factors for 10 partial instantiations in the network per second, the running times using an exhaustive search algorithm would be:

```{r echo=FALSE}
df <- tibble(
  `Number of Nodes` = c(5, 5, 10, 20),
  `States per Node` = c(5, 10, 10, 10),
  `Total Combinations` = c("5^5 = 3125", "10^5 = 100,000", "10^10 = 10,000,000,000", "10^20 = 100,000,000,000,000,000,000"),
  `Expected Runtime` = c("~0.005 minutes", "~0.167 minutes", "~31.7 years", "~3.17 billion years")
)

knitr::kable(df)
```


As you can see, the expected runtime increases dramatically as the number of nodes and states per node increase. 
This is why exhaustive search becomes computationally infeasible for even fairly small networks of only 10 nodes.


## Hierarchical Beam Search

The Hierarchical Beam Search algorithm is proposed by Changhe Yuan and Xiaoyuan Zhu in <i><a href="https://www.sciencedirect.com/science/article/pii/S1570868316300854">'Hierarchical beam search for solving most relevant explanation in Bayesian networks</i></a>, Journal of Applied Logic, Volume 22, 2017, Pages 3-13' as an approximate inference algorithm that performs remarkably well on a suite of Bayesian network benchmarks and works by applying multi-stage scan-and-prune logic, pruning candidate hypotheses that don't improve the Generalized Bayes Factor and gorwing a blacklist of hypotheses from which to cull successive candidate hypotheses until every Generalized Bayes Factor in the search space has either been explicitly evaluated or pruned. This reduction in the search space + pruning criteria combination is what makes it possible for this algorithm to enable reliable inference but also be computationally feasible on even large networks. 

This graphic taken from their paper describes the search algorithm nicely:

<img src="./images/HBS Graphic.gif"/>
<br/>

In the graphic we can see the inner circle contains nodes "a", "A", "b", "B", "c", and "C". If we imagine each of these characters to be one hypothesis (e.g. "a" = 'Reckless Driver == TRUE', "b" = 'Reckless Driver == FALSE' etc) then the first beam in the search would generate Generalized Bayes Factors for each individual candidate hypothesis and select only those with a GBF that meets our criteria for a relevant explanation, and we can then reduce the search space in the second outer circle to expanded hypotheses from the viable hypotheses in the inner circle. Lets say for example that after our first scan of the search space (the inner most circle) of hypotheses, we discover that node "a" is the only viable explanation; then we can reduce the search space of the second circle of hypotheses to only the expanded hypotheses from "a", e.g. "reckless driver == TRUE", and then we combine "a" with "c" (which we pruned in the first beam) to create a second level hypothesis: "(reckless driver == TRUE) AND (breathes oxygen == FALSE). Each pass focuses on adding one layer to the hypothesis until we have either evaluated the GBF of or pruned away every hypothesis. What we will be left with is the final set of hypotheses in the Bayesian network that maximizes the Generalized Bayes Factor.


# The MRE R Package

At the time of writing on June 8th 2023 there is no other open source implementation for calculating the Most Relevant Explanation in a Bayesian network, either exhaustively or through the proposed Hierarchical Beam Search. The `MRE` R package implements the Hierarchical Beam Search algorithm proposed by Yuan and Zhu for calculating the Most Relevant Explanation in a Bayesian network.

# Example Usage

Here is another example where we can surface the Most Relevant Explanation ignoring any spurious associations that would otherwise be deemed important under a maximum likelihood / correlation framework.

```{r message=FALSE}
# load mre
library(mre)

# create a synthetic dataset of age, gender and income
set.seed(42)
n <- 10000
age <- factor(sample(c("young", "adolescent", "old"), n, replace = TRUE, prob = c(0.33, 0.33, 0.34)))
gender <- factor(sample(c("male", "nonbinary", "female"), n, replace = TRUE, prob = c(0.33, 0.33, 0.34)))

# define probabilities for income based on age and gender
income_probs <- function(age_val, gender_val) {
  if (age_val == "old" & gender_val == "male") {
    return(c("low" = 0.01, "high" = 0.99))
  } else if (age_val == "young" & gender_val == "female") {
    return(c("low" = 0.99, "high" = 0.01))
  } else {
    return(c("low" = 0.99, "high" = 0.01))
  }
}

# generate income variable
income <- factor(mapply(function(a, g) {
  sample(c("low", "high"), 1, replace = TRUE, prob = income_probs(a, g))
}, age, gender))

# join age, gender and income into a dataframe
raw_data <- data.frame(age, gender, income)

# define the targets that you want explanations for 
target_var = "income"
target_val = "high"

# run
mre_solution <- mre::mre_hierarchical_beam_search(
  x = raw_data, 
  target_variable = target_var, 
  target_value = target_val
  )

# show the candidate hypotheses with the greatest Generalized Bayes Factors
knitr::kable(mre_solution)
```


# Installation

To install the `mre` package, run the following line:

```{r eval=FALSE}
devtools::install_github("davelorino/mre")
```














